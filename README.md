https://samarkanov.info/blog/2026/feb/Running-Local-LLMs-In-February-2026.html

Today I'm investigating inference speed and quality of local LLM models. I'll be running a set of models on a Linux VPS (CPU only, no GPUs) and Apple Silicon (M2 and M4) to gain insights about inference speed, intelligence of models.

![](entry/static/models-ls.png)
